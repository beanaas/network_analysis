{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "with open('./links/2.txt', 'r') as file:\n",
    "    # Read and process each line\n",
    "    for line in file:\n",
    "        splitted = line.split(\" \")\n",
    "        n = int(splitted[0])\n",
    "        nprime = int(splitted[1])\n",
    "        links.append((n, nprime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import scipy.sparse as sp\n",
      "rows, cols = zip(*[(i - 1, j - 1) for i, j in links])\n",
      "data = np.ones(len(rows), dtype=int)\n",
      "print(max(rows), max(cols))\n",
      "\n",
      "# Create the sparse matrix using coo_matrix\n",
      "size = len(list(titles.keys()))\n",
      "sparse_matrix = sp.coo_matrix((data, (rows, cols)), shape=(size, size), dtype=np.float64)\n",
      "\n",
      "# Calculate the eigenvector centrality\n",
      "eigen_value, eigen_vector = sp.linalg.eigs(sparse_matrix.T,k=1)  # Use transpose for right eigenvector\n",
      "\n",
      "\n",
      "print(titles[np.argmax(eigen_vector)])\n",
      "links = []\n",
      "with open('./links/2.txt', 'r') as file:\n",
      "    # Read and process each line\n",
      "    for line in file:\n",
      "        splitted = line.split(\" \")\n",
      "        n = int(splitted[0])\n",
      "        nprime = int(splitted[1])\n",
      "        links.append((n, nprime))\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "titles = {}\n",
    "with open('./titles/2.txt', 'r') as file:\n",
    "    # Read and process each line\n",
    "    for line in file:\n",
    "        counter+=1\n",
    "        titles[counter] = line\n",
    "degrees_in = {}\n",
    "degrees_out = {}\n",
    "\n",
    "for nnprime in links:\n",
    "    n = nnprime[0]\n",
    "    nprime = nnprime[1]\n",
    "\n",
    "    if(n in degrees_out.keys()):\n",
    "        degrees_out[n].append(nprime)\n",
    "    else:\n",
    "        degrees_out[n] = [nprime]\n",
    "\n",
    "    if(nprime in degrees_in.keys()):\n",
    "        degrees_in[nprime].append(n)\n",
    "    else:\n",
    "        degrees_in[nprime] = [n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_degrees_in = {}\n",
    "num_degrees_out = {}\n",
    "for i in degrees_in:\n",
    "    num_degrees_in[i] = len(degrees_in[i])\n",
    "for i in degrees_out:\n",
    "    num_degrees_out[i] = len(degrees_out[i])\n",
    "sorted_degrees_in = dict(sorted(num_degrees_in.items(), key=lambda x: x[1], reverse=True))\n",
    "sorted_degrees_out = dict(sorted(num_degrees_out.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "node_ids_list1 = []\n",
    "node_ids_list2 = []\n",
    "\n",
    "normalized_in_degree = {}\n",
    "normalized_out_degree = {}\n",
    "\n",
    "\n",
    "for i in sorted_degrees_in:\n",
    "    node_ids_list1.append(i)\n",
    "    normalized_in_degree[i] = num_degrees_in[i]/(len(links))\n",
    "    if(i in sorted_degrees_out.keys()):\n",
    "        list1.append((degrees_in[i], degrees_out[i]))\n",
    "    else:\n",
    "        list1.append((degrees_in[i], []))\n",
    "\n",
    "for i in sorted_degrees_out:\n",
    "    normalized_out_degree[i] = num_degrees_out[i]/(len(links))\n",
    "    node_ids_list2.append(i)\n",
    "    if(i in sorted_degrees_in.keys()):\n",
    "        list2.append((degrees_in[i], degrees_out[i])) \n",
    "    else:\n",
    "        list2.append(([], degrees_out[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN DEGREES\n",
      "2007\n",
      " 0.007685 0.000254\n",
      "2008\n",
      " 0.006608 1.2e-05\n",
      "United_States\n",
      " 0.006596 1.2e-05\n",
      "United_Kingdom\n",
      " 0.00541 0.000218\n",
      "France\n",
      " 0.005156 0.001476\n",
      "OUT DEGREES\n",
      ".cf\n",
      " 0.003231 4.8e-05\n",
      "Book_of_Optics\n",
      " 0.003171 0.000242\n",
      "Bengali_language\n",
      " 0.002844 0.000351\n",
      "Dumfries,_VA\n",
      " 0.002759 1.2e-05\n",
      "Ahmad_Zarruq\n",
      " 0.002638 0.001404\n"
     ]
    }
   ],
   "source": [
    "print(\"IN DEGREES\")\n",
    "for node_id in node_ids_list1[:5]:\n",
    "    title = titles[node_id]\n",
    "    print(title, round(normalized_in_degree[node_id],6), round(normalized_out_degree[node_id],6))\n",
    "\n",
    "print(\"OUT DEGREES\")\n",
    "for node_id in node_ids_list2[:5]:\n",
    "    title = titles[node_id]\n",
    "    print(title, round(normalized_out_degree[node_id],6), round(normalized_in_degree[node_id],6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUB\n",
      ".cf\n",
      " 0.006991 0.0\n",
      "Book_of_Optics\n",
      " 0.006978 7.8e-05\n",
      "Bengali_language\n",
      " 0.006846 0.00014\n",
      "Erich_Schmidt_(archaeologist)\n",
      " 0.006811 1e-06\n",
      "1999_NATO_bombing_of_the_Federal_Republic_of_Yugoslavia\n",
      " 0.006803 4e-06\n",
      "AUTH\n",
      "Shams_Tabrizi\n",
      " 0.006591 0.000277\n",
      "Mahmud_Shabistari\n",
      " 0.00658 0.0003\n",
      "Al-Ghazali\n",
      " 0.006541 0.0\n",
      "Shahab_al-Din_Suhrawardi\n",
      " 0.006472 0.0\n",
      "Farid_al-Din_Attar\n",
      " 0.00646 0.0\n"
     ]
    }
   ],
   "source": [
    "def normalize(dictionary):\n",
    "    tot = sum(dictionary.values())\n",
    "    for key in dictionary.keys():\n",
    "        dictionary[key] = dictionary[key]/tot\n",
    "    return dictionary\n",
    "\n",
    "def get_new_hub_score(degrees_out, auths):\n",
    "    #iterate over all hubs to update\n",
    "    new_hub = {}\n",
    "    for hub_node in degrees_out.keys():\n",
    "        #gets all the nodes pointing to hub node\n",
    "        new_hub_score = 0\n",
    "        for auth_node in degrees_out[hub_node]:\n",
    "            new_hub_score += auths[auth_node]\n",
    "        new_hub[hub_node] = new_hub_score\n",
    "    return normalize(new_hub)\n",
    "\n",
    "def get_new_auth_score(degrees_in, hubs):\n",
    "    new_auth = {}\n",
    "    for auth_node in degrees_in.keys():\n",
    "        new_auth_score = 0\n",
    "        for out_node in degrees_in[auth_node]:\n",
    "            new_auth_score += hubs[out_node]\n",
    "        new_auth[auth_node] = new_auth_score\n",
    "    return normalize(new_auth)\n",
    "\n",
    "hubs = normalized_out_degree\n",
    "auths = normalized_in_degree\n",
    "\n",
    "for i in range(100):\n",
    "    hubs = get_new_hub_score(degrees_out, auths)\n",
    "    auths = get_new_auth_score(degrees_in, hubs)\n",
    "\n",
    "\n",
    "sorted_on_hub_score = dict(sorted(hubs.items(), key=lambda x: x[1], reverse=True))\n",
    "sorted_on_auth_score = dict(sorted(auths.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "top5_hubs = list(sorted_on_hub_score.keys())[:5]\n",
    "top5_auths = list(sorted_on_auth_score.keys())[:5]\n",
    "print(\"HUB\")\n",
    "for i in top5_hubs:\n",
    "    if(i not in sorted_on_auth_score.keys()):\n",
    "        print(titles[i], round(sorted_on_hub_score[i],6), 0.00)\n",
    "    else:\n",
    "        print(titles[i], round(sorted_on_hub_score[i],6), round(sorted_on_auth_score[i],6))\n",
    "\n",
    "print(\"AUTH\")\n",
    "for i in top5_auths:\n",
    "    if(i not in sorted_on_hub_score.keys()):\n",
    "        print(titles[i], round(sorted_on_auth_score[i],6), 0.00)\n",
    "    else:\n",
    "        print(titles[i], round(sorted_on_auth_score[i],6), round(sorted_on_hub_score[i],6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUB\n",
      "United_States\n",
      " 0.007305 0.0\n",
      "United_Kingdom\n",
      " 0.005992 0.0\n",
      "France\n",
      " 0.00571 0.0\n",
      "Iran\n",
      " 0.005228 0.0\n",
      "Wikimedia_Commons\n",
      " 0.004946 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_eigenvector(degrees_in, centrality_in):\n",
    "    for node in degrees_in.keys():\n",
    "        centrality = 0\n",
    "        for node_in in degrees_in[node]:\n",
    "            if(node_in in centrality_in.keys()):\n",
    "                centrality += centrality_in[node_in]\n",
    "        centrality_in[i] = centrality\n",
    "    return centrality_in\n",
    "\n",
    "centrality_in = num_degrees_in\n",
    "        \n",
    "for i in range(200):\n",
    "    centrality_in = get_eigenvector(degrees_in, centrality_in)\n",
    "\n",
    "centrality_in = normalize(centrality_in)\n",
    "sorted_on_vector_score = dict(sorted(centrality_in.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "top5_centrality = list(sorted_on_vector_score.keys())[:5]\n",
    "print(\"HUB\")\n",
    "for i in top5_centrality:\n",
    "    print(titles[i], round(sorted_on_vector_score[i],6), 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 2999\n",
      "2007\n",
      " 0.006334\n",
      "United_States\n",
      " 0.006312\n",
      "2008\n",
      " 0.005851\n",
      "France\n",
      " 0.005535\n",
      "United_Kingdom\n",
      " 0.005324\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "rows, cols = zip(*[(i - 1, j - 1) for i, j in links])\n",
    "data = np.ones(len(rows), dtype=int)\n",
    "print(max(rows), max(cols))\n",
    "\n",
    "# Create the sparse matrix using coo_matrix\n",
    "size = len(list(titles.keys()))\n",
    "sparse_matrix = sp.coo_matrix((data, (rows, cols)), shape=(size, size), dtype=np.float64)\n",
    "\n",
    "# Calculate the eigenvector centrality\n",
    "eigen_value, eigen_vector = sp.linalg.eigs(sparse_matrix.T,k=1, which=\"LR\")  # Use transpose for right eigenvector\n",
    "\n",
    "eigen_vector = np.abs(eigen_vector.flatten())\n",
    "eigen_vector /= eigen_vector.sum()\n",
    "\n",
    "sorted_indices = np.argsort(eigen_vector)[::-1]\n",
    "\n",
    "\n",
    "# Get the values and indices of the first 5 elements\n",
    "top5_eigen = eigen_vector[sorted_indices[:5]]\n",
    "top5_eigen_indices = sorted_indices[:5]\n",
    "for i in top5_eigen_indices:\n",
    "    print(titles[i+1], round(eigen_vector[i],6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
